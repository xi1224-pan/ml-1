{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOB/SY60/u4Z0p1Z0SQ9zmz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xi1224-pan/ml-1/blob/main/%E8%A8%93%E7%B7%B4%E5%A5%A7%E5%BE%B7%E8%B3%BD(%E5%B7%B2%E8%A8%93%E7%B7%B4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn --quiet\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# 1. 讀取已標註的資料\n",
        "df = pd.read_csv(\"/content/奧德賽標註.csv\")\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. 載入 tokenizer 與模型（這裡仍用 roberta，num_labels=3）\n",
        "model_name = \"uer/roberta-base-finetuned-jd-binary-chinese\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3, ignore_mismatched_sizes=True)\n",
        "\n",
        "# 3. Tokenize 資料\n",
        "def tokenize_fn(texts, labels):\n",
        "    encoding = tokenizer(list(texts), padding=True, truncation=True, max_length=64, return_tensors=\"pt\")\n",
        "    encoding[\"labels\"] = torch.tensor(list(labels), dtype=torch.long)  # <- 明確轉換為 list\n",
        "    return encoding\n",
        "\n",
        "\n",
        "train_enc = tokenize_fn(train_df[\"text\"], train_df[\"label\"])\n",
        "val_enc = tokenize_fn(val_df[\"text\"], val_df[\"label\"])\n",
        "\n",
        "# 4. 建立 DataLoader\n",
        "train_dataset = torch.utils.data.TensorDataset(train_enc[\"input_ids\"], train_enc[\"attention_mask\"], train_enc[\"labels\"])\n",
        "val_dataset = torch.utils.data.TensorDataset(val_enc[\"input_ids\"], val_enc[\"attention_mask\"], val_enc[\"labels\"])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "# 5. 訓練設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 6. 開始訓練\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1} - Train loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # 驗證階段\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch {epoch+1} - Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "# 7. 儲存模型\n",
        "model.save_pretrained(\"/mnt/data/mario_sentiment_model\")\n",
        "tokenizer.save_pretrained(\"/mnt/data/mario_sentiment_model\")\n"
      ],
      "metadata": {
        "id": "UtvgFAiX_U0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62df8bdf-5901-4653-cf08-dbb772a11490",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at uer/roberta-base-finetuned-jd-binary-chinese and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 9/9 [00:00<00:00,  9.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train loss: 1.0602\n",
            "Epoch 1 - Validation Accuracy: 0.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 9/9 [00:00<00:00, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Train loss: 0.9356\n",
            "Epoch 2 - Validation Accuracy: 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 9/9 [00:00<00:00, 10.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Train loss: 0.8494\n",
            "Epoch 3 - Validation Accuracy: 0.8889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 9/9 [00:00<00:00, 10.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Train loss: 0.7005\n",
            "Epoch 4 - Validation Accuracy: 0.8889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 9/9 [00:00<00:00, 10.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Train loss: 0.5637\n",
            "Epoch 5 - Validation Accuracy: 0.9444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 9/9 [00:00<00:00, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Train loss: 0.4058\n",
            "Epoch 6 - Validation Accuracy: 0.9444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 9/9 [00:00<00:00, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Train loss: 0.2603\n",
            "Epoch 7 - Validation Accuracy: 0.9444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 9/9 [00:00<00:00, 10.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Train loss: 0.1888\n",
            "Epoch 8 - Validation Accuracy: 0.8889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 9/9 [00:00<00:00, 10.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Train loss: 0.0858\n",
            "Epoch 9 - Validation Accuracy: 0.8889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 9/9 [00:00<00:00, 10.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Train loss: 0.0499\n",
            "Epoch 10 - Validation Accuracy: 0.9444\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/mnt/data/mario_sentiment_model/tokenizer_config.json',\n",
              " '/mnt/data/mario_sentiment_model/special_tokens_map.json',\n",
              " '/mnt/data/mario_sentiment_model/vocab.txt',\n",
              " '/mnt/data/mario_sentiment_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "collapsed": true,
        "id": "DpHC7G0itNSm",
        "outputId": "ec276cda-6910-41be-a374-51beb615fb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "2    30\n",
              "1    30\n",
              "0    30\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas numpy torch --quiet\n",
        "# 重新載入必要模組並執行模型初始化與分析邏輯\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. 載入模型與 tokenizer\n",
        "model_path = \"/mnt/data/mario_sentiment_model\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2. 定義情緒分析函式\n",
        "def predict_with_score(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "        score = probs[0][2].item() - probs[0][0].item()  # 正面 - 負面\n",
        "        label = torch.argmax(probs, dim=1).item()\n",
        "    label_map = {0: \"負面\", 1: \"中立\", 2: \"正面\"}\n",
        "    return {\n",
        "        \"情緒分類\": label_map[label],\n",
        "        \"情緒分數\": round(score, 4)\n",
        "    }\n",
        "\n",
        "# 3. 分析整份檔案（逐行分析）\n",
        "def analyze_file(df, text_column=\"text\"):\n",
        "    results = []\n",
        "    for _, row in df.iterrows():\n",
        "        analysis = predict_with_score(row[text_column])\n",
        "        results.append({\n",
        "            \"句子\": row[text_column],\n",
        "            \"情緒分類\": analysis[\"情緒分類\"],\n",
        "            \"情緒分數\": analysis[\"情緒分數\"]\n",
        "        })\n",
        "\n",
        "    # 總結\n",
        "    scores = [r[\"情緒分數\"] for r in results]\n",
        "    avg_score = np.mean(scores)\n",
        "    if avg_score > 1:\n",
        "        overall = \"結論為強烈正面\"\n",
        "    elif avg_score > 0:\n",
        "        overall = \"結論為正面\"\n",
        "    elif avg_score == 0:\n",
        "        overall = \"結論為中立\"\n",
        "    else:\n",
        "        overall = \"結論為負面\"\n",
        "\n",
        "    results.append({\n",
        "        \"句子\": \"【總結】\",\n",
        "        \"情緒分類\": overall,\n",
        "        \"情緒分數\": round(avg_score, 4)\n",
        "    })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "# 匯入資料\n",
        "df_input = pd.read_csv(\"/content/hk4g4.csv\")  # 確保有一欄是 \"text\"\n",
        "\n",
        "# 執行分析\n",
        "result_df = analyze_file(df_input, text_column=\"text\")\n",
        "\n",
        "# 匯出結果\n",
        "result_df.to_csv(\"/mnt/data/情緒分析結果.csv\", index=False, encoding=\"utf-8-sig\")\n"
      ],
      "metadata": {
        "id": "GFXBCX-00Sdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762d9b22-33f1-4aa4-c8c3-2b6e1caac1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    }
  ]
}